"""
TrustMed AI - Chainlit Application wired to AWS Bedrock Knowledge Base.
"""

import asyncio
import json
import os
from pathlib import Path
from typing import List, Tuple
from urllib.parse import urlparse

import boto3
from botocore.exceptions import BotoCoreError, ClientError
import chainlit as cl

BANNER_AUTHOR = "AI"
AWS_REGION = os.getenv("AWS_REGION", "us-east-1")
BEDROCK_KB_ID = os.getenv("BEDROCK_KB_ID","CVSWBQ5BFR")
BEDROCK_MODEL_ARN = "meta.llama3-8b-instruct-v1:0"
# BEDROCK_MODEL_ARN = "mistral.mistral-small-2402-v1:0"
# BEDROCK_MODEL_ARN = os.getenv("BEDROCK_MODEL_ARN")
KB_VECTOR_RESULTS = int(os.getenv("KB_VECTOR_RESULTS", "8"))
TOPIC_FILTER_ENABLED = os.getenv("ENABLE_TOPIC_FILTER", "false").lower() in (
    "1",
    "true",
    "yes",
)
SUPPORTED_KEYWORDS = (
    "diabetes",
    "type 2",
    "type ii",
    "prediabetes",
    "hyperglycemia",
    "hypoglycemia",
    "glucose",
    "a1c",
    "blood sugar",
    "insulin",
    "metformin",
    "sulfonylurea",
    "glp-1",
    "heart",
    "cardio",
    "cardiovascular",
    "cardiology",
    "blood pressure",
    "hypertension",
    "beta blocker",
    "ace inhibitor",
    "statin",
    "cholesterol",
    "triglyceride",
    "arrhythmia",
    "stroke",
    "lifestyle",
    "diet",
    "exercise",
    "medication",
    "medicine",
    "drug",
    "symptom",
    "risk factor",
)

PROMPT_TEMPLATE = """
<system>
You are TrustMed AI, a retrieval-augmented educator for cardiometabolic health topics
covered in this proof-of-concept (Type II Diabetes, cardiovascular disease, and related
therapies). Provide empathetic, plain-language explanations grounded in authoritative
medical literature.
- Cite only the authoritative documents retrieved from the knowledge base. Do not cite
  or rely on Reddit/forums for medical facts; forum content is allowed solely to decode
  colloquial language or patient slang.
- Summarize actionable guidance (risk factors, diagnostics, lifestyle, medications) and
  call out when a licensed clinician should be consulted.
- If the search results lack sufficient evidence, state that limitation explicitly
  instead of speculating.
- If the user asks about questions that are not related to the knowledge base, politely inform them that you are not able to answer that question.
Current time: $current_time$
</system>

<conversation_goal>
User intent: $query$
</conversation_goal>

<retrieved_authoritative_sources>
$search_results$
</retrieved_authoritative_sources>

<formatting_requirements>
$output_format_instructions$
</formatting_requirements>

<response_expectations>
1. Begin with a single-sentence overview tailored to the user's concern.
2. Expand with concise paragraphs or bullets that translate clinical concepts into
   everyday language.
3. Tie each factual claim to the supporting authoritative source inline (e.g., "(Mayo Clinic)")
   that aligns with the autogenerated citation list.
4. Do not create a standalone "Sources" section or hyperlink list—the application appends
   its own sources block.
5. Recommend follow-up with healthcare professionals when appropriate.
</response_expectations>
""".strip()

_bedrock_client = None

_default_manifest_path = (
    Path(os.getenv("CONTENT_MANIFEST_PATH"))
    if os.getenv("CONTENT_MANIFEST_PATH")
    else Path(__file__).resolve().parent.parent / "to_upload" / "manifest.json"
)
CONTENT_PREFIX = os.getenv("CONTENT_PREFIX", "to_upload/")


def load_manifest(path: Path) -> dict:
    if not path.exists():
        return {}
    try:
        entries = json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError:
        return {}

    lookup = {}
    prefix = CONTENT_PREFIX.rstrip("/")
    for entry in entries:
        key = entry.get("key")
        if not key:
            continue
        lookup[key] = entry
        if prefix:
            lookup[f"{prefix}/{key}"] = entry
    return lookup


MANIFEST_LOOKUP = load_manifest(_default_manifest_path)


def get_bedrock_client():
    global _bedrock_client
    if _bedrock_client is None:
        _bedrock_client = boto3.client(
            "bedrock-agent-runtime",
            region_name=AWS_REGION,
        )
    return _bedrock_client


def format_citations(citations: List[dict]) -> Tuple[str, List[cl.Text]]:
    """
    Convert the citations returned by Bedrock into a markdown block and Chainlit elements.
    """
    if not citations:
        return "", []

    citation_lines = []
    source_elements: List[cl.Text] = []
    source_idx = 1
    seen_sources = set()

    for citation in citations:
        for ref in citation.get("retrievedReferences", []):
            snippet = ref.get("content", {}).get("text") or "No snippet available."
            s3_uri = ref.get("location", {}).get("s3Location", {}).get("uri")
            filename = s3_uri.split("/")[-1] if s3_uri else f"source_{source_idx}"

            manifest_entry = None
            if s3_uri:
                parsed = urlparse(s3_uri)
                if parsed.scheme == "s3":
                    s3_path = parsed.path.lstrip("/")
                    manifest_entry = MANIFEST_LOOKUP.get(s3_path)
                    if not manifest_entry:
                        _, _, key_only = s3_path.partition("/")
                        if key_only:
                            manifest_entry = MANIFEST_LOOKUP.get(key_only)

            display_name = (
                manifest_entry.get("title") if manifest_entry else filename
            ) or filename
            canonical_url = (
                manifest_entry.get("canonical_url") if manifest_entry else s3_uri
            ) or s3_uri

            entry_type = (
                manifest_entry.get("type", "").lower() if manifest_entry else ""
            )
            entry_key = manifest_entry.get("key", "").lower() if manifest_entry else ""
            normalized_link = (canonical_url or s3_uri or filename or "").lower()
            is_manifest_file = any(
                value and value.lower().endswith("manifest.json")
                for value in (display_name, canonical_url, filename)
            )
            refers_to_forum = any(
                value
                for value in (
                    entry_type == "forum",
                    "forums/" in entry_key,
                    "/forums/" in (s3_uri or "").lower(),
                    "reddit.com" in (canonical_url or "").lower(),
                )
            )
            if is_manifest_file or refers_to_forum:
                continue
            if normalized_link in seen_sources:
                continue
            seen_sources.add(normalized_link)

            if canonical_url:
                link_text = f"[{display_name}]({canonical_url})"
            else:
                link_text = display_name

            citation_lines.append(f"{source_idx}. {link_text}")
            # source_elements.append(
            #     cl.Text(
            #         name=f"Source {source_idx}",
            #         content=snippet,
            #         display="inline",
            #     )
            # )
            source_idx += 1

    citation_block = ""
    if citation_lines:
        citation_block = "\n\n**Sources:**\n" + "\n".join(citation_lines)
    return citation_block, source_elements


def needs_retry_response(answer_text: str, citations: List[dict]) -> bool:
    """
    Determine if we should retry the Bedrock call without the custom prompt.
    """
    normalized = (answer_text or "").strip().lower()
    no_citations = not citations

    refusal_phrases = (
        "sorry, i am unable",
        "sorry, i'm unable",
        "i am unable to assist",
        "i'm unable to assist",
        "cannot assist",
        "can't assist",
    )
    refused = any(phrase in normalized for phrase in refusal_phrases)

    return not normalized or refused or no_citations


def build_retrieve_payload(user_text: str, include_prompt_template: bool = True) -> dict:
    """
    Construct the RetrieveAndGenerate request payload.
    """
    kb_configuration = {
        "knowledgeBaseId": BEDROCK_KB_ID,
        "modelArn": BEDROCK_MODEL_ARN,
        "retrievalConfiguration": {
            "vectorSearchConfiguration": {
                "numberOfResults": KB_VECTOR_RESULTS,
                "overrideSearchType": "HYBRID",
            }
        },
    }

    if include_prompt_template:
        kb_configuration["generationConfiguration"] = {
            "promptTemplate": {
                "textPromptTemplate": PROMPT_TEMPLATE,
            }
        }

    return {
        "input": {"text": user_text},
        "retrieveAndGenerateConfiguration": {
            "type": "KNOWLEDGE_BASE",
            "knowledgeBaseConfiguration": kb_configuration,
        },
    }


def is_supported_query(text: str) -> bool:
    """
    Basic heuristic to keep the assistant focused on cardiometabolic topics.
    """
    if not text:
        return False
    lowered = text.lower()
    return any(keyword in lowered for keyword in SUPPORTED_KEYWORDS)


@cl.on_chat_start
async def on_chat_start():
    """
    Called when a new chat session starts.
    """
    # Read the chainlit.md file and send it as welcome message
    chainlit_md_path = Path(__file__).parent / "chainlit.md"
    if chainlit_md_path.exists():
        welcome_content = chainlit_md_path.read_text(encoding="utf-8")
    else:
        welcome_content = """
# Welcome to TrustMed AI! 

Ask about Type II Diabetes, Heart Disease, medications, or symptoms to see the
RAG pipeline retrieve citations from both authoritative and forum sources.
        """
    
    cl.user_session.set("bedrock_session_id", None)

    await cl.Message(
        content=welcome_content,
        author=BANNER_AUTHOR,
    ).send()


@cl.on_message
async def on_message(message: cl.Message):
    """
    Called when a user sends a message. Streams the prompt through Bedrock's
    retrieve_and_generate API and returns the grounded answer with citations.
    """
    response_msg = cl.Message(author=BANNER_AUTHOR, content="")
    await response_msg.send()

    missing_configs = []
    if not BEDROCK_KB_ID:
        missing_configs.append("BEDROCK_KB_ID")
    if not BEDROCK_MODEL_ARN:
        missing_configs.append("BEDROCK_MODEL_ARN")

    if missing_configs:
        response_msg.content = (
            "⚠️ Missing configuration: "
            + ", ".join(missing_configs)
            + ". Please export these environment variables and restart Chainlit."
        )
        await response_msg.update()
        return

    try:
        # Provide a minimal delay so the UI shows a spinner even for fast responses.
        await asyncio.sleep(0.2)

        if TOPIC_FILTER_ENABLED and not is_supported_query(message.content):
            response_msg.content = (
                "TrustMed AI answers questions about cardiometabolic health topics "
                "such as Type II Diabetes, heart disease, blood pressure, and related "
                "medications. I’m not able to help with that request."
            )
            await response_msg.update()
            return

        client = get_bedrock_client()
        bedrock_session_id = cl.user_session.get("bedrock_session_id")

        primary_payload = build_retrieve_payload(
            message.content, include_prompt_template=True
        )
        if bedrock_session_id:
            primary_payload["sessionId"] = bedrock_session_id

        bedrock_response = client.retrieve_and_generate(**primary_payload)
        new_session_id = bedrock_response.get("sessionId")

        answer_text = bedrock_response.get("output", {}).get(
            "text",
            "No response text was returned by Bedrock.",
        )
        citations = bedrock_response.get("citations", [])

        if needs_retry_response(answer_text, citations):
            fallback_payload = build_retrieve_payload(
                message.content, include_prompt_template=False
            )
            session_for_retry = bedrock_session_id or new_session_id
            if session_for_retry:
                fallback_payload["sessionId"] = session_for_retry

            fallback_response = client.retrieve_and_generate(**fallback_payload)
            new_session_id = fallback_response.get("sessionId") or new_session_id
            answer_text = fallback_response.get("output", {}).get(
                "text",
                "No response text was returned by Bedrock.",
            )
            citations = fallback_response.get("citations", [])

        if new_session_id:
            cl.user_session.set("bedrock_session_id", new_session_id)

        citation_block, source_elements = format_citations(citations)

        response_msg.content = answer_text + citation_block
        # response_msg.elements = source_elements
    except (ClientError, BotoCoreError) as aws_err:
        response_msg.content = f"⚠️ Bedrock error: {aws_err}"
    except Exception as unexpected_err:  # pragma: no cover
        response_msg.content = f"⚠️ Unexpected error: {unexpected_err}"

    await response_msg.update()


@cl.on_stop
async def on_stop():
    """
    Called when the user stops the conversation.
    """
    await cl.Message(
        content="Session ended. Thank you for using TrustMed AI!",
        author=BANNER_AUTHOR,
    ).send()

